# cis4340webscrape

For the non-AI version of this project, I used Beautiful Soup for web scraping. I chose this library because I was already familiar with it from completing assignments in ISC4551: Data Graphics and Visualization this semester. Beautiful Soup is straightforward and efficient for extracting data, often requiring only a few lines of code to locate and parse relevant HTML elements. One of the major advantages of using Beautiful Soup is how seamlessly it integrates with pandas, allowing scraped data to be quickly converted into DataFrames for easier cleaning, manipulation, and analysis.

Although Scrapy offers a more comprehensive framework with advanced features such as asynchronous scraping, built-in crawling capabilities, and data pipelines, it also introduces significantly more overhead and complexity. For larger-scale scraping projects that require navigating multiple pages, handling sessions, or maintaining structured pipelines, Scrapy would be a strong choice. However, given the relatively small scale of this project, Beautiful Soup provided a more accessible and flexible solution. It gave me greater control over each step of the scraping process without the need to learn or manage the heavier architecture that comes with Scrapy. Ultimately, its simplicity, familiarity, and effectiveness made it the better fit for my needs in this case.

In approaching the ScrapeGraphAI component of the project, I chose to follow a structure similar to the template I had already developed using Beautiful Soup. Since I was already comfortable with the logic I had built in that earlier phase, it provided a reliable foundation to work from. The dataset itself presented several intricacies, including inconsistencies in formatting, varying table structures, and the lack of a straightforward path for cleaning and integration. Because of this, trying to overhaul the approach or use a completely new structure might have introduced additional complexity and potential for errors. By maintaining a similar structure to the Beautiful Soup method, I was able to minimize the risk of data misinterpretation or structural mismatches. One of the key decisions I made was to have the AI process each table individually rather than attempt to combine them during the initial scraping phase. This approach allowed me to apply the same simpler logic I had already refined with Beautiful Soup.

I figured the best way to convey my needs to AI was to have AI tell it what to do. Using ChatGPT, I told it to write a prompt for ScrapeGraphAI. Specifically, please write a prompt for ScrapeGraphAI to extract the four tables from list of boosters on the Wikipedia page keeping the four tables separate, with a link to the Wikipedia page. It returned to me, Go to: https://en.wikipedia.org/wiki/List_of_Falcon_9_first-stage_boosters#List_of_boosters . Task: From the section titled "List of boosters", extract the four tables under that section. For each table: Extract all rows and columns exactly as displayed on the page. Keep the tables separate. Store the result as a list of four Pandas DataFrames called frames: frames[0] is the first table under the section, frames[1] is the second table, frames[2] is the third, frames[3] is the fourth. Create and store these four DataFrames in a Jupyter notebook.

The Python development tool I employed for this project is Jupyter Notebook. Having used it extensively in various other classes, I already had many of the necessary libraries installed, which made setup more convenient. One of the features I particularly appreciate about Jupyter Notebook is its use of code cells. The ability to run code one cell at a time offers a more interactive and modular development experience. Although this approach can lengthen the overall process of running a complete program compared to executing a script all at once, it greatly enhances debugging and experimentation. By isolating and testing individual parts of the code, I can quickly identify issues, monitor outputs step by step, and make targeted adjustments without rerunning the entire script. I used the command prompt to convert the Jupyter files to Python files.

Developing the functions for capturing the selected data was simple. However this is the result of extensive cleaning of the data. When I took in the data, it was seperated by table. Each table presented its own unique quirks and collection of data, making it easier to handle. Instead of having complex logic to handle various cases across all the tables, I could create simplier logic to handle the values of each table. As we only want information for launches, I removed any rows for engines that did not launch. In the first data frame for engine B0002 and B1002, the line says there were so many test flights, without a row for each launch. Standardizing all the column names made it easier to manipulate the data. I dropped the payload column as it was irrevelant data for our purposes. The format of the dates needed to be changed to match the format (yyyy-mm-dd). Converting it to datetime variable type, made it more difficult for CSV as it was stubborn sticking Excel's preferred formatting for dates. The previously mentioned multiple test flights was a range of dates rather than a singular date, so this made to be handled differently. The launch and landing sites were only part of the data contained within that cell. There was also information about whether it was a failure, success, etc. The sites had to be extracted using regex. Though this had to filtered for instances where no landing was attempted. For fate, each entry took on the final fate. For all entries except the last launch for an engine, this value had to be changed to reflect the engine's return to service. Flight and block type was interesting to tackle. Information in one column had to be split into two. This also required extra logic as some cells included only flight type and block type had to be implied. The data couldn't simply be split into two. Some entries had extra info that in the Wikipedia page hyperlinked. This had to be cleaned. The last thing was to combine all the data frames to make for simplier quering. Overall it was a tedious process, but so is most detail-oriented work.

![image](https://github.com/user-attachments/assets/40c22af5-6268-451c-8b59-a0716aca379a)
